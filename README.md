# mlp_pytorch
Multilayer Perceptron for regression problems using PyTorch

This perceptron was created with three layers (one input, one hidden, and one output).
The hidden layer uses the sigmoid activation function, although ReLU can also be used.
It has a class for training and validation, incorporating early stopping as a simple method to avoid overfitting of the network.
In addition, it has a class for data normalization prior to training.
